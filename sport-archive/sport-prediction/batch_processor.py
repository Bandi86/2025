#!/usr/bin/env python3
"""
SZERENCSEMIX T√ñMEGES PDF FELDOLGOZ√ì
==================================

C√©lja: A teljes PDF arch√≠vum feldolgoz√°sa nagy mennyis√©gben,
robusztus hibakezel√©ssel √©s halad√°s k√∂vet√©ssel.

Funkci√≥k:
1. Teljes arch√≠vum felt√©rk√©pez√©se √©s priorit√°si sorba rendez√©se
2. Batch feldolgoz√°s hibakezel√©ssel √©s √∫jrapr√≥b√°lkoz√°ssal
3. Folyamat k√∂vet√©s √©s jelent√©s k√©sz√≠t√©s
4. Automatikus adatmin≈ës√©g ellen≈ërz√©s
5. Le√°ll√≠that√≥ √©s folytathat√≥ feldolgoz√°s

Haszn√°lat:
    python batch_processor.py --batch-size 50 --max-workers 4
    python batch_processor.py --resume --from-date 2024-01-01
    python batch_processor.py --priority recent --limit 100

Verzi√≥: 1.0
D√°tum: 2025-06-28
"""

import sqlite3
import argparse
import json
import time
import sys
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
import re
import logging

# Relat√≠v importok
from real_pdf_processor import RealPdfProcessor
from data_cleaner import SzerencseMixDataCleaner
from data_loader_pipeline import DatabaseLoader

@dataclass
class ProcessingJob:
    """Egyetlen PDF feldolgoz√°si feladat"""
    pdf_path: str
    priority: int  # 1=legmagasabb, 10=legalacsonyabb
    estimated_date: Optional[str]
    file_size: int
    last_attempt: Optional[str] = None
    attempt_count: int = 0
    status: str = "pending"  # pending, processing, completed, failed, skipped
    error_message: Optional[str] = None
    processing_time: Optional[float] = None
    matches_extracted: int = 0

@dataclass
class BatchProgress:
    """Batch feldolgoz√°s √°llapota"""
    total_files: int
    completed: int
    failed: int
    skipped: int
    in_progress: int
    start_time: datetime
    estimated_completion: Optional[datetime] = None
    current_rate: float = 0.0  # files per minute

class SzerencseMixBatchProcessor:
    def __init__(self, archive_path: str, db_path: str, log_level: str = "INFO"):
        self.archive_path = Path(archive_path)
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row

        # Logging setup
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('batch_processing.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

        # PDF feldolgoz√≥ √©s adatbet√∂lt≈ë
        self.pdf_processor = RealPdfProcessor()
        self.data_loader = DatabaseLoader(db_path)

        # Progress tracking
        self.progress_file = Path("batch_progress.json")
        self.jobs: List[ProcessingJob] = []
        self.progress: Optional[BatchProgress] = None

    def discover_pdfs(self, priority_strategy: str = "chronological") -> List[ProcessingJob]:
        """PDF f√°jlok felder√≠t√©se √©s priorit√°si sorba rendez√©se"""
        self.logger.info(f"PDF f√°jlok felder√≠t√©se: {self.archive_path}")

        if not self.archive_path.exists():
            raise FileNotFoundError(f"Arch√≠vum mappa nem tal√°lhat√≥: {self.archive_path}")

        # √ñsszes PDF f√°jl keres√©se
        pdf_files = list(self.archive_path.rglob("*.pdf"))
        self.logger.info(f"Tal√°lt PDF f√°jlok: {len(pdf_files)}")

        jobs = []
        for pdf_file in pdf_files:
            job = ProcessingJob(
                pdf_path=str(pdf_file),
                priority=self._calculate_priority(pdf_file, priority_strategy),
                estimated_date=self._extract_date_from_filename(pdf_file.name),
                file_size=pdf_file.stat().st_size
            )
            jobs.append(job)

        # Priorit√°s szerinti rendez√©s
        jobs.sort(key=lambda x: (x.priority, x.estimated_date or "0000-00-00"), reverse=True)

        self.logger.info(f"Feldolgoz√°si sorrend meghat√°rozva: {len(jobs)} f√°jl")
        return jobs

    def _calculate_priority(self, pdf_file: Path, strategy: str) -> int:
        """Priorit√°s sz√°m√≠t√°s k√ºl√∂nb√∂z≈ë strat√©gi√°k szerint"""
        if strategy == "recent":
            # √öjabb f√°jlok magasabb priorit√°ssal
            date_str = self._extract_date_from_filename(pdf_file.name)
            if date_str:
                try:
                    date_obj = datetime.strptime(date_str, "%Y-%m-%d")
                    days_ago = (datetime.now() - date_obj).days
                    return max(1, 10 - min(days_ago // 30, 9))  # 1-10 sk√°la
                except:
                    pass
            return 5

        elif strategy == "size":
            # Kisebb f√°jlok magasabb priorit√°ssal (gyorsabb)
            size_kb = pdf_file.stat().st_size / 1024
            if size_kb < 100:
                return 9
            elif size_kb < 500:
                return 7
            elif size_kb < 1000:
                return 5
            else:
                return 3

        elif strategy == "chronological":
            # R√©gebbiek alacsonyabb priorit√°ssal
            date_str = self._extract_date_from_filename(pdf_file.name)
            if date_str:
                try:
                    date_obj = datetime.strptime(date_str, "%Y-%m-%d")
                    days_ago = (datetime.now() - date_obj).days
                    return min(10, max(1, days_ago // 30))
                except:
                    pass
            return 5

        return 5  # default priority

    def _extract_date_from_filename(self, filename: str) -> Optional[str]:
        """D√°tum kinyer√©se a f√°jln√©vb≈ël"""
        # K√ºl√∂nb√∂z≈ë d√°tum form√°tumok keres√©se
        patterns = [
            r'(\d{4})\.(\d{2})\.(\d{2})',  # 2025.06.27
            r'(\d{4})-(\d{2})-(\d{2})',   # 2025-06-27
            r'(\d{2})-(\d{2})_(\d{4})',   # 06-27_2025
        ]

        for pattern in patterns:
            match = re.search(pattern, filename)
            if match:
                groups = match.groups()
                if len(groups) == 3:
                    if len(groups[0]) == 4:  # YYYY-MM-DD vagy YYYY.MM.DD
                        return f"{groups[0]}-{groups[1]}-{groups[2]}"
                    else:  # MM-DD_YYYY
                        return f"{groups[2]}-{groups[0]}-{groups[1]}"

        return None

    def check_already_processed(self) -> List[str]:
        """M√°r feldolgozott f√°jlok list√°ja"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT DISTINCT source_pdf FROM historical_matches WHERE source_pdf IS NOT NULL")
        processed_historical = [row[0] for row in cursor.fetchall()]

        cursor.execute("SELECT DISTINCT source_pdf FROM future_matches WHERE source_pdf IS NOT NULL")
        processed_future = [row[0] for row in cursor.fetchall()]

        cursor.execute("SELECT pdf_filename FROM extraction_logs WHERE status = 'completed'")
        processed_logs = [row[0] for row in cursor.fetchall()]

        all_processed = set(processed_historical + processed_future + processed_logs)
        self.logger.info(f"M√°r feldolgozott f√°jlok: {len(all_processed)}")
        return list(all_processed)

    def filter_jobs(self, jobs: List[ProcessingJob],
                   skip_processed: bool = True,
                   from_date: Optional[str] = None,
                   to_date: Optional[str] = None,
                   limit: Optional[int] = None) -> List[ProcessingJob]:
        """Feladatok sz≈±r√©se"""

        if skip_processed:
            processed_files = self.check_already_processed()
            processed_basenames = {Path(f).name for f in processed_files}
            jobs = [job for job in jobs if Path(job.pdf_path).name not in processed_basenames]
            self.logger.info(f"Sz≈±r√©s ut√°n: {len(jobs)} √∫j f√°jl")

        if from_date:
            jobs = [job for job in jobs if not job.estimated_date or job.estimated_date >= from_date]
            self.logger.info(f"D√°tum sz≈±r√©s ut√°n (>= {from_date}): {len(jobs)} f√°jl")

        if to_date:
            jobs = [job for job in jobs if not job.estimated_date or job.estimated_date <= to_date]
            self.logger.info(f"D√°tum sz≈±r√©s ut√°n (<= {to_date}): {len(jobs)} f√°jl")

        if limit:
            jobs = jobs[:limit]
            self.logger.info(f"Limit alkalmazva: {len(jobs)} f√°jl")

        return jobs

    def process_batch(self, jobs: List[ProcessingJob],
                     batch_size: int = 10,
                     max_workers: int = 2,
                     max_retries: int = 3) -> BatchProgress:
        """Batch feldolgoz√°s v√©grehajt√°sa"""

        self.jobs = jobs
        self.progress = BatchProgress(
            total_files=len(jobs),
            completed=0,
            failed=0,
            skipped=0,
            in_progress=0,
            start_time=datetime.now()
        )

        self.logger.info(f"Batch feldolgoz√°s kezd√©se: {len(jobs)} f√°jl, {max_workers} worker")

        # Batch-ek l√©trehoz√°sa
        batches = [jobs[i:i + batch_size] for i in range(0, len(jobs), batch_size)]

        for batch_idx, batch in enumerate(batches):
            self.logger.info(f"Batch {batch_idx + 1}/{len(batches)} feldolgoz√°sa ({len(batch)} f√°jl)")

            # P√°rhuzamos feldolgoz√°s
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(self._process_single_pdf, job): job for job in batch}

                for future in as_completed(futures):
                    job = futures[future]
                    try:
                        result = future.result()
                        self._update_progress(job, result)
                    except Exception as e:
                        self.logger.error(f"Hiba a {job.pdf_path} feldolgoz√°s√°ban: {e}")
                        job.status = "failed"
                        job.error_message = str(e)
                        self.progress.failed += 1

            # Progress ment√©se
            self._save_progress()

            # Adatmin≈ës√©g ellen≈ërz√©s minden 5. batch ut√°n
            if (batch_idx + 1) % 5 == 0:
                self._run_quality_check()

        self.logger.info("Batch feldolgoz√°s befejezve")
        return self.progress

    def _process_single_pdf(self, job: ProcessingJob) -> Dict:
        """Egyetlen PDF feldolgoz√°sa"""
        start_time = time.time()
        job.status = "processing"
        job.last_attempt = datetime.now().isoformat()
        job.attempt_count += 1

        self.logger.debug(f"Feldolgoz√°s: {job.pdf_path}")

        try:
            # PDF feldolgoz√°s - meccsek kinyer√©se
            pdf_path = Path(job.pdf_path)
            result = self.pdf_processor.process_pdf(pdf_path)

            if result and result.get('success', False):
                # Adatok bet√∂lt√©se az adatb√°zisba
                future_matches = result.get('future_matches', [])
                historical_matches = result.get('historical_matches', [])

                # Log kezd√©se
                log_id = self.data_loader.log_extraction(pdf_path.name, str(pdf_path), 'processing')

                try:
                    # Bet√∂lt√©s az adatb√°zisba
                    inserted_future = 0
                    for match in future_matches:
                        if self.data_loader.load_future_match(match, pdf_path.name, confidence=0.8):
                            inserted_future += 1

                    inserted_historical = 0
                    for match in historical_matches:
                        if self.data_loader.load_historical_match(match, pdf_path.name, confidence=0.8):
                            inserted_historical += 1

                    total_inserted = inserted_future + inserted_historical

                    # Log lez√°r√°sa
                    self.data_loader.update_extraction_log(
                        log_id,
                        'completed',
                        {
                            'records_extracted': total_inserted,
                            'matches_found': len(future_matches) + len(historical_matches),
                            'future_matches': inserted_future,
                            'historical_matches': inserted_historical
                        }
                    )

                    job.status = "completed"
                    job.matches_extracted = total_inserted
                    self.logger.info(f"Sikeres: {pdf_path.name} ({total_inserted} meccs mentve)")

                except Exception as db_error:
                    self.data_loader.update_extraction_log(log_id, 'failed', {'error': str(db_error)})
                    raise db_error

            else:
                job.status = "failed"
                job.error_message = result.get('error', 'PDF feldolgoz√°s sikertelen')
                self.logger.warning(f"Sikertelen: {pdf_path.name} - {job.error_message}")

        except Exception as e:
            job.status = "failed"
            job.error_message = str(e)
            self.logger.error(f"Kiv√©tel: {Path(job.pdf_path).name} - {e}")

        job.processing_time = time.time() - start_time
        return {"job": job, "success": job.status == "completed"}

    def _update_progress(self, job: ProcessingJob, result: Dict):
        """Progress friss√≠t√©se"""
        if result["success"]:
            self.progress.completed += 1
        else:
            self.progress.failed += 1

        # Sebess√©g sz√°m√≠t√°s
        elapsed = (datetime.now() - self.progress.start_time).total_seconds() / 60
        if elapsed > 0:
            self.progress.current_rate = (self.progress.completed + self.progress.failed) / elapsed

            # Becsl√©s a befejez√©sre
            remaining = self.progress.total_files - self.progress.completed - self.progress.failed
            if self.progress.current_rate > 0:
                eta_minutes = remaining / self.progress.current_rate
                self.progress.estimated_completion = datetime.now() + timedelta(minutes=eta_minutes)

    def _save_progress(self):
        """Progress ment√©se JSON f√°jlba"""
        progress_data = {
            "progress": asdict(self.progress),
            "jobs": [asdict(job) for job in self.jobs],
            "last_updated": datetime.now().isoformat()
        }

        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, indent=2, ensure_ascii=False, default=str)

    def load_progress(self) -> bool:
        """Mentett progress bet√∂lt√©se"""
        if not self.progress_file.exists():
            return False

        try:
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            self.progress = BatchProgress(**data["progress"])
            self.jobs = [ProcessingJob(**job_data) for job_data in data["jobs"]]

            self.logger.info(f"Progress bet√∂ltve: {len(self.jobs)} feladat")
            return True
        except Exception as e:
            self.logger.error(f"Progress bet√∂lt√©se sikertelen: {e}")
            return False

    def _run_quality_check(self):
        """Adatmin≈ës√©g ellen≈ërz√©s"""
        self.logger.info("Adatmin≈ës√©g ellen≈ërz√©s...")
        try:
            cleaner = SzerencseMixDataCleaner(self.db_path)
            metrics = cleaner.update_quality_metrics()
            cleaner.close()

            self.logger.info(f"Min≈ës√©gi metrik√°k: {metrics}")
        except Exception as e:
            self.logger.error(f"Min≈ës√©g ellen≈ërz√©s hiba: {e}")

    def generate_report(self) -> str:
        """R√©szletes jelent√©s gener√°l√°sa"""
        if not self.progress:
            return "Nincs feldolgoz√°si adat."

        report = []
        report.append("=" * 60)
        report.append("SZERENCSEMIX BATCH FELDOLGOZ√ÅSI JELENT√âS")
        report.append("=" * 60)

        # √ñsszes√≠t≈ë statisztik√°k
        report.append(f"\nüìä √ñSSZES√çT≈ê:")
        report.append(f"   √ñsszes f√°jl: {self.progress.total_files}")
        report.append(f"   Sikeres: {self.progress.completed}")
        report.append(f"   Sikertelen: {self.progress.failed}")
        report.append(f"   Kihagyott: {self.progress.skipped}")
        report.append(f"   Feldolgoz√°s alatt: {self.progress.in_progress}")

        success_rate = (self.progress.completed / self.progress.total_files * 100) if self.progress.total_files > 0 else 0
        report.append(f"   Sikeress√©gi ar√°ny: {success_rate:.1f}%")

        # Id≈ëz√≠t√©s
        elapsed = datetime.now() - self.progress.start_time
        report.append(f"\n‚è±Ô∏è ID≈êZ√çT√âS:")
        report.append(f"   Kezd√©s: {self.progress.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"   Eltelt id≈ë: {elapsed}")
        report.append(f"   Jelenlegi sebess√©g: {self.progress.current_rate:.1f} f√°jl/perc")

        if self.progress.estimated_completion:
            report.append(f"   Becs√ºlt befejez√©s: {self.progress.estimated_completion.strftime('%Y-%m-%d %H:%M:%S')}")

        # Hib√°k elemz√©se
        failed_jobs = [job for job in self.jobs if job.status == "failed"]
        if failed_jobs:
            report.append(f"\n‚ùå HIB√ÅK ({len(failed_jobs)}):")
            error_counts = {}
            for job in failed_jobs:
                error_type = job.error_message.split(':')[0] if job.error_message else "Ismeretlen"
                error_counts[error_type] = error_counts.get(error_type, 0) + 1

            for error_type, count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True):
                report.append(f"   {error_type}: {count} eset")

        # Top performerek
        completed_jobs = [job for job in self.jobs if job.status == "completed"]
        if completed_jobs:
            top_jobs = sorted(completed_jobs, key=lambda x: x.matches_extracted, reverse=True)[:5]
            report.append(f"\nüèÜ LEGJOBB TAL√ÅLATOK:")
            for job in top_jobs:
                filename = Path(job.pdf_path).name
                report.append(f"   {filename}: {job.matches_extracted} meccs")

        return "\n".join(report)

    def close(self):
        """Er≈ëforr√°sok felszabad√≠t√°sa"""
        if self.conn:
            self.conn.close()
        if hasattr(self, 'pdf_processor'):
            # Assuming pdf_processor has a close method
            pass

def main():
    parser = argparse.ArgumentParser(description='SzerencseMix batch PDF processor')
    parser.add_argument('--archive', default='data/szerencsemix_archive', help='PDF arch√≠vum mappa')
    parser.add_argument('--db', default='data/football_database.db', help='Adatb√°zis f√°jl')
    parser.add_argument('--batch-size', type=int, default=10, help='Batch m√©ret')
    parser.add_argument('--max-workers', type=int, default=2, help='P√°rhuzamos worker-ek sz√°ma')
    parser.add_argument('--priority', choices=['recent', 'chronological', 'size'], default='recent', help='Priorit√°si strat√©gia')
    parser.add_argument('--limit', type=int, help='Maximum feldolgozand√≥ f√°jlok sz√°ma')
    parser.add_argument('--from-date', help='Kezd≈ë d√°tum (YYYY-MM-DD)')
    parser.add_argument('--to-date', help='Befejez≈ë d√°tum (YYYY-MM-DD)')
    parser.add_argument('--resume', action='store_true', help='Kor√°bbi feldolgoz√°s folytat√°sa')
    parser.add_argument('--skip-processed', action='store_true', default=True, help='M√°r feldolgozott f√°jlok kihagy√°sa')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO', help='Log szint')

    args = parser.parse_args()

    # Adatb√°zis ellen≈ërz√©se
    if not Path(args.db).exists():
        print(f"‚ùå Adatb√°zis nem tal√°lhat√≥: {args.db}")
        return 1

    processor = SzerencseMixBatchProcessor(args.archive, args.db, args.log_level)

    try:
        # Progress bet√∂lt√©se ha --resume
        if args.resume and processor.load_progress():
            print("üì• Kor√°bbi feldolgoz√°s bet√∂ltve")
            jobs = processor.jobs
        else:
            # PDF-ek felder√≠t√©se
            print("üîç PDF f√°jlok felder√≠t√©se...")
            jobs = processor.discover_pdfs(args.priority)

            # Sz≈±r√©s
            jobs = processor.filter_jobs(
                jobs,
                skip_processed=args.skip_processed,
                from_date=args.from_date,
                to_date=args.to_date,
                limit=args.limit
            )

        if not jobs:
            print("‚úÖ Nincs feldolgozand√≥ f√°jl")
            return 0

        print(f"üöÄ {len(jobs)} f√°jl feldolgoz√°sa kezd≈ëdik...")

        # Batch feldolgoz√°s
        progress = processor.process_batch(
            jobs,
            batch_size=args.batch_size,
            max_workers=args.max_workers
        )

        # Jelent√©s
        report = processor.generate_report()
        print("\n" + report)

        # Jelent√©s ment√©se
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"batch_report_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"\nüíæ Jelent√©s mentve: {report_file}")

        return 0 if progress.failed == 0 else 1

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Feldolgoz√°s megszak√≠tva felhaszn√°l√≥ √°ltal")
        processor._save_progress()
        return 130
    except Exception as e:
        print(f"\n‚ùå Kritikus hiba: {e}")
        return 1
    finally:
        processor.close()

if __name__ == "__main__":
    sys.exit(main())
