import requests
from bs4 import BeautifulSoup
from bs4.element import NavigableString
import json
from datetime import datetime, timezone
from collections import defaultdict
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# Magyar h√≥napnevek a d√°tum form√°z√°shoz
HUNGARIAN_MONTHS = {
    1: 'janu√°r', 2: 'febru√°r', 3: 'm√°rcius', 4: '√°prilis',
    5: 'm√°jus', 6: 'j√∫nius', 7: 'j√∫lius', 8: 'augusztus',
    9: 'szeptember', 10: 'okt√≥ber', 11: 'november', 12: 'december'
}

def format_hungarian_date(date_str, time_str):
    """Konvert√°lja a d√°tumot magyar form√°tumba"""
    try:
        # Pr√≥b√°ljuk meg k√ºl√∂nb√∂z≈ë form√°tumokkal
        for date_format in ['%d.%m.', '%d.%m.%Y', '%Y-%m-%d']:
            try:
                if date_format == '%d.%m.':
                    # Hozz√°adjuk az aktu√°lis √©vet
                    current_year = datetime.now().year
                    full_date_str = f"{date_str}{current_year}"
                    parsed_date = datetime.strptime(full_date_str, '%d.%m.%Y')
                else:
                    parsed_date = datetime.strptime(date_str, date_format)
                break
            except ValueError:
                continue
        else:
            return f"{date_str} {time_str}"

        # Magyar form√°tum: 2025. j√∫lius 8., 18:00
        month_name = HUNGARIAN_MONTHS[parsed_date.month]
        return f"{parsed_date.year}. {month_name} {parsed_date.day}., {time_str}"
    except:
        return f"{date_str} {time_str}"

def parse_page_title_info(page_title):
    """Kinyeri az inform√°ci√≥kat a page title-b≈ël"""
    info = {
        'score': '',
        'home_team': '',
        'away_team': '',
        'competition': ''
    }

    if '|' in page_title:
        # Form√°tum: "THO 1-4 WES | Thornton Redbacks - West Wallsend"
        parts = page_title.split('|')

        if len(parts) >= 2:
            # Bal oldal: eredm√©ny r√©sszel
            left_part = parts[0].strip()
            # Jobb oldal: teljes csapat nevek
            right_part = parts[1].strip()

            # Eredm√©ny keres√©se a bal oldalban
            score_match = re.search(r'(\d+[-:]\d+)', left_part)
            if score_match:
                info['score'] = score_match.group(1)

            # Csapat nevek a jobb oldalb√≥l
            if ' - ' in right_part:
                team_parts = right_part.split(' - ')
                info['home_team'] = team_parts[0].strip()
                info['away_team'] = team_parts[1].strip()

    return info

def detect_event_type_advanced(element_text, parent_classes=None):
    """Fejlettebb esem√©ny t√≠pus felismer√©s"""
    text = element_text.lower()

    # G√≥l felismer√©se
    if any(keyword in text for keyword in ['goal', 'g√≥l', 'score']):
        return 'goal', 'G√≥l'

    # K√°rty√°k felismer√©se
    if any(keyword in text for keyword in ['yellow', 's√°rga', 'sarga']):
        return 'yellow_card', 'S√°rga lap'
    elif any(keyword in text for keyword in ['red', 'piros']):
        return 'red_card', 'Piros lap'

    # Csere felismer√©se
    if any(keyword in text for keyword in ['substitution', 'sub', 'csere', '‚Üí', '->', '‚Üî']):
        return 'substitution', 'Csere'

    # Oszt√°lyok alap√∫ felismer√©s
    if parent_classes:
        class_text = ' '.join(parent_classes).lower()
        if 'goal' in class_text:
            return 'goal', 'G√≥l'
        elif 'card' in class_text:
            if 'yellow' in class_text:
                return 'yellow_card', 'S√°rga lap'
            elif 'red' in class_text:
                return 'red_card', 'Piros lap'

    # Ha id≈ë van benne, val√≥sz√≠n≈±leg esem√©ny
    if re.search(r'\d+[:\'+]', text):
        return 'event', 'Esem√©ny'

    return 'unknown', 'Ismeretlen'

def extract_events_from_container(container, debug=False):
    """Esem√©nyek kinyer√©se egy kont√©nerb≈ël"""
    events = []

    # Keress√ºk meg az √∂sszes elemet ami id≈ët tartalmaz
    time_elements = container.select('[class*="time"], p.time, .i-field.time, [class*="minute"]')

    if debug:
        print(f"Found {len(time_elements)} time elements")

    for time_elem in time_elements:
        # Sz√ºl≈ë elem keres√©se ami tartalmazhatja az eg√©sz esem√©nyt
        event_container = time_elem.parent
        if not event_container:
            continue

        # Id≈ë kinyer√©se
        time_text = time_elem.get_text(strip=True)
        time_match = re.search(r'(\d+)[:\'+]?', time_text)
        time = time_match.group(1) + "'" if time_match else time_text

        # Az eg√©sz esem√©ny sz√∂vege
        full_event_text = event_container.get_text(strip=True)

        # Esem√©ny t√≠pus felismer√©se
        event_type, description = detect_event_type_advanced(
            full_event_text,
            event_container.get('class', [])
        )

        # J√°t√©kos n√©v kinyer√©se - mindent ami nem id≈ë
        player_parts = re.split(r'\d+[:\'+]', full_event_text)
        player_text = ''.join(player_parts).strip()

        # Csapat jel√∂l√©s elt√°vol√≠t√°sa [BOL], [IND] stb.
        player_text = re.sub(r'\[.*?\]', '', player_text).strip()

        # Extra karakterek elt√°vol√≠t√°sa
        player_text = re.sub(r'[()]+', '', player_text).strip()

        if player_text and len(player_text) > 1:
            events.append({
                'time': time,
                'type': event_type,
                'description': description,
                'player': player_text,
                'raw_text': full_event_text if debug else None
            })

    return events

def scrape_match_details_v2(match_url, debug=True):
    """Tov√°bbfejlesztett meccs r√©szletek scraping-je"""
    print(f"Scraping details from: {match_url}")

    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)

    match_details = {
        'header_info': {
            'main_title': '',
            'league_round': '',
            'date_time_hungarian': '',
            'date': '',
            'time': '',
            'final_score': '',
            'home_team': '',
            'away_team': ''
        },
        'summary': [],
        'statistics': {},
        'lineups': {},
        'debug_info': {} if debug else None
    }

    try:
        driver.get(match_url)
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

        # V√°runk egy kicsit hogy bet√∂lts√∂n minden
        import time
        time.sleep(2)

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # === HEADER INFORM√ÅCI√ìK KINYER√âSE ===

        # 1. Page title elemz√©se
        page_title = soup.title.get_text() if soup.title else ''
        title_info = parse_page_title_info(page_title)

        match_details['header_info']['main_title'] = page_title
        match_details['header_info']['final_score'] = title_info['score']
        match_details['header_info']['home_team'] = title_info['home_team']
        match_details['header_info']['away_team'] = title_info['away_team']

        # 2. D√°tum √©s id≈ë keres√©se a body sz√∂veg√©ben
        body_text = soup.get_text()

        # D√°tum form√°tumok: "08.07.2025 12:00" vagy "08.07. 12:00"
        datetime_match = re.search(r'(\d{1,2}\.\d{1,2}\.(?:\d{4})?)\s+(\d{1,2}:\d{2})', body_text)
        if datetime_match:
            date_str = datetime_match.group(1)
            time_str = datetime_match.group(2)

            match_details['header_info']['date'] = date_str
            match_details['header_info']['time'] = time_str
            match_details['header_info']['date_time_hungarian'] = format_hungarian_date(date_str, time_str)

        # 3. Liga/verseny inform√°ci√≥ keres√©se
        # Keress√ºk meg a f≈ëc√≠meket ami nem GDPR vonatkoz√°s√∫
        headings = soup.select('h1, h2, h3, h4')
        for heading in headings:
            text = heading.get_text(strip=True)
            if (text and
                len(text) > 5 and
                'consent' not in text.lower() and
                'cookie' not in text.lower() and
                'privacy' not in text.lower() and
                'data' not in text.lower()):
                match_details['header_info']['league_round'] = text
                break

        # === ESEM√âNYEK KINYER√âSE ===

        # Keress√ºk meg a f≈ë tartalom kont√©nert
        main_content = soup.select_one('#detail-tab-content, .match-content, .match-events, body')

        if main_content:
            events = extract_events_from_container(main_content, debug)
            match_details['summary'] = events

            if debug:
                match_details['debug_info']['events_extracted'] = len(events)
                match_details['debug_info']['sample_events'] = events[:3] if events else []

        if debug:
            match_details['debug_info'].update({
                'page_title': page_title,
                'url': match_url,
                'title_info_parsed': title_info,
                'datetime_found': bool(datetime_match),
                'main_content_found': bool(main_content)
            })

    except Exception as e:
        print(f"Error during scraping: {e}")
        if debug:
            match_details['debug_info']['error'] = str(e)

    finally:
        driver.quit()

    return match_details

def scrape_and_process_results_v2():
    """Tov√°bbfejlesztett f≈ë funkci√≥"""
    base_url = "https://m.eredmenyek.com"
    url = f"{base_url}/"
    response = requests.get(url)
    raw_results = []

    if response.status_code != 200:
        print(f"Failed to fetch the page. Status code: {response.status_code}")
        return

    soup = BeautifulSoup(response.text, 'html.parser')
    score_data = soup.find('div', id='score-data')

    if not score_data:
        print("Could not find the main score data container.")
        return

    current_league = "Unknown"
    match_links = score_data.find_all('a', class_=['sched', 'fin', 'live'])

    for link in match_links:
        league_tag = link.find_previous('h4')
        if league_tag:
            current_league = league_tag.get_text(strip=True).replace("Tabella", "").strip()

        crawler = link.previous_sibling
        teams_text = ""
        while crawler and hasattr(crawler, 'name') and crawler.name != 'span':
            if isinstance(crawler, NavigableString):
                teams_text = str(crawler) + teams_text
            crawler = crawler.previous_sibling

        teams_text = teams_text.strip()

        if crawler and hasattr(crawler, 'name') and crawler.name == 'span':
            time_or_status = crawler.get_text(strip=True)
            score = link.get_text(strip=True)

            # Status class kinyer√©se biztons√°gosan
            status_classes = link.get('class', [])
            status_class = status_classes[0] if status_classes else 'unknown'

            detail_path = link.get('href')
            detail_page_url = f"{base_url}{detail_path}" if detail_path else None

            teams_text = teams_text.replace('\n', ' ').replace('    ', ' ').strip()
            team_parts = teams_text.split(' - ')
            if len(team_parts) < 2:
                team_parts = teams_text.split('-')

            home_team = team_parts[0].strip() if len(team_parts) > 0 else 'N/A'
            away_team = team_parts[1].strip() if len(team_parts) > 1 else 'N/A'

            raw_results.append({
                'league': current_league,
                'time_status': time_or_status,
                'home_team': home_team,
                'away_team': away_team,
                'score': score,
                'match_status': status_class,
                'detail_page_url': detail_page_url
            })

    if not raw_results:
        print("No matches found.")
        return

    def get_sort_key(match):
        time_str = match['time_status'].split('T√∂r√∂lve')[0].split('Elhalasztva')[0]
        try:
            if "'" in time_str:
                return f"00:{time_str.replace("'", "")}"
            return datetime.strptime(time_str, '%H:%M').strftime('%H:%M')
        except ValueError:
            return "99:99"

    sorted_matches = sorted(raw_results, key=get_sort_key)

    total_matches = len(sorted_matches)
    friendly_matches = sum(1 for m in sorted_matches if 'Bar√°ts√°gos' in m['league'])
    matches_by_league = defaultdict(int)
    for m in sorted_matches:
        matches_by_league[m['league']] += 1

    statistics = {
        'total_matches': total_matches,
        'friendly_matches': friendly_matches,
        'matches_by_league': dict(matches_by_league)
    }

    grouped_matches = {
        'live': [m for m in sorted_matches if m['match_status'] == 'live'],
        'finished': [m for m in sorted_matches if m['match_status'] == 'fin'],
        'scheduled': [m for m in sorted_matches if m['match_status'] == 'sched']
    }

    final_output = {
        'metadata': {
            'source': url,
            'scrape_timestamp_utc': datetime.now(timezone.utc).isoformat() + 'Z',
            'date_for_matches': datetime.now().strftime('%Y-%m-%d')
        },
        'statistics': statistics,
        'matches': grouped_matches
    }

    list_file_name = f"v2_{datetime.now().strftime('%Y-%m-%d')}.json"
    with open(list_file_name, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    print(f"Successfully scraped and processed {total_matches} matches.")
    print(f"List of matches saved to {list_file_name}")

    # --- Scrape details for the first finished match ---
    first_finished_match = next((m for m in grouped_matches['finished'] if m['detail_page_url']), None)

    if first_finished_match:
        print(f"\n=== Scraping detailed match info ===")
        print(f"Match: {first_finished_match['home_team']} vs {first_finished_match['away_team']}")
        print(f"League: {first_finished_match['league']}")
        print(f"Score: {first_finished_match['score']}")

        details_url = first_finished_match['detail_page_url']
        match_details_data = scrape_match_details_v2(details_url, debug=True)

        if match_details_data:
            details_file_name = f"v2_{first_finished_match['home_team'].replace(' ', '_')}-vs-{first_finished_match['away_team'].replace(' ', '_')}_details.json"
            with open(details_file_name, 'w', encoding='utf-8') as f:
                json.dump(match_details_data, f, indent=4, ensure_ascii=False)
            print(f"Match details saved to {details_file_name}")

            # Eredm√©nyek √∂sszefoglal√°sa
            print(f"\n=== EREDM√âNYEK √ñSSZEFOGLAL√ÅSA ===")
            header = match_details_data['header_info']

            if header['main_title']:
                print(f"üìÑ F≈ëc√≠m: {header['main_title']}")
            if header['league_round']:
                print(f"üèÜ Liga/Fordul√≥: {header['league_round']}")
            if header['date_time_hungarian']:
                print(f"üìÖ Magyar d√°tum: {header['date_time_hungarian']}")
            if header['final_score']:
                print(f"‚öΩ Eredm√©ny: {header['final_score']}")
            if header['home_team'] and header['away_team']:
                print(f"üÜö Csapatok: {header['home_team']} vs {header['away_team']}")

            print(f"üìã Esem√©nyek sz√°ma: {len(match_details_data['summary'])}")

            # Els≈ë n√©h√°ny esem√©ny megjelen√≠t√©se
            if match_details_data['summary']:
                print(f"\nüìù Els≈ë esem√©nyek:")
                for i, event in enumerate(match_details_data['summary'][:5]):
                    print(f"  {i+1}. {event['time']} - {event['description']} ({event['player']})")

                if len(match_details_data['summary']) > 5:
                    print(f"  ... √©s m√©g {len(match_details_data['summary'])-5} esem√©ny")

            print(f"=" * 50)
    else:
        print("No finished matches with details found to scrape.")

if __name__ == "__main__":
    scrape_and_process_results_v2()
